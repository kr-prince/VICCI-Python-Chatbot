{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_train_data",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwzGPtBmBp8q"
      },
      "source": [
        "### Pre-requisite Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk8HhHNjzCBp"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2wB57xQzNA9"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxQ5AGZJ93jb"
      },
      "source": [
        "### Training data Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76Wh_V9ByfpI"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from random import sample, choices, choice\n",
        "from transformers import AutoModelWithLMHead, AutoTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGO6U3bC6oGr"
      },
      "source": [
        "# Reading the manually curated data\n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/VICCI/data/hand_written_train_data.json'\n",
        "training_data = None\n",
        "with open(file_path, 'r') as file:\n",
        "    training_data = json.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkq10yTkWojm"
      },
      "source": [
        "# Reading the names of states \n",
        "file_path = '/content/drive/MyDrive/Colab Notebooks/VICCI/data/state_names.txt'\n",
        "STATE_LIST = None\n",
        "with open(file_path, 'r', encoding='utf-8-sig') as file:\n",
        "    STATE_LIST = [sname.strip().lower() for sname in file.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpFtpGAZpBZb",
        "outputId": "446a2241-4d1a-41f7-f9ad-7fd9b1e18dbe"
      },
      "source": [
        "training_data[10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intent': 'spread',\n",
              " 'query': ['How does COVID-19 spread?',\n",
              "  'How is COVID spreading?',\n",
              "  'How can I get infected?',\n",
              "  'What can I catch COVID?',\n",
              "  'What is causing the virus to spread so fast?',\n",
              "  'How is it spreading so fast?',\n",
              "  'How can COVID-19 spread?',\n",
              "  'How does this virus spreads?',\n",
              "  'How is the virus spreading?',\n",
              "  'How can I catch COVID?'],\n",
              " 'response': ['People can catch COVID-19 from others who have the virus. The disease can spread from person to person through small droplets from the nose or mouth which are spread when a person with COVID-19 coughs or exhales. These droplets land on objects and surfaces around the person. Other people then catch COVID-19 by touching these objects or surfaces, then touching their eyes, nose or mouth. People can also catch COVID-19 if they breathe in droplets from a person with COVID-19 who coughs out or exhales droplets. This is why it is important to stay more than 1 meter (3 feet) away from a person who is sick.']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfo4q1gvzWlp",
        "outputId": "3887ae1a-f325-46ce-98aa-a161c6bf1c84"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-small-finetuned-quora-for-paraphrasing\")\n",
        "model = AutoModelWithLMHead.from_pretrained(\"mrm8488/t5-small-finetuned-quora-for-paraphrasing\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:847: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hev-6MdFzcqa"
      },
      "source": [
        "# Huggingface article by Manuel Romero\n",
        "# https://huggingface.co/mrm8488/t5-small-finetuned-quora-for-paraphrasing\n",
        "\n",
        "def paraphrase(text, num_sequences=5, max_length=128):\n",
        "    # We increase the num_beams to 10\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
        "    generated_ids = model.generate(input_ids=input_ids, num_return_sequences=num_sequences, \n",
        "                        num_beams=10, max_length=max_length, no_repeat_ngram_size=2, \n",
        "                        repetition_penalty=3.5, length_penalty=1.0, early_stopping=True)\n",
        "    preds = [tokenizer.decode(g, skip_special_tokens=True, \n",
        "                              clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "    return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dYVtJauy58X"
      },
      "source": [
        "training_data_big=[]\n",
        "for train_set in training_data:\n",
        "    query_list = []\n",
        "    if train_set['intent'] in ['greet','bye','thanks','intro']:\n",
        "        # For these category of intents, the input questions are very short and crisp but the \n",
        "        # question phrases generated are too long and carry unnecessary context which is not relevant for us.\n",
        "        # So we simply oversample our own training data\n",
        "        query_list.extend(choices(train_set['query'], k=50))\n",
        "    else:\n",
        "        query_pool=set()\n",
        "        # For other intents we collect all the paraphrases generated for each of our hand written questions\n",
        "        for query in train_set['query']:\n",
        "            # Some of the paraphrases are only case-different, so we convert them to lower case and take the set\n",
        "            query_pool.update([phrase.lower() for phrase in paraphrase(\"paraphrase: \"+query, num_sequences=5)])\n",
        "        \n",
        "        # if the question pool has more than 50 generated paraphrases we randomly sample any 50\n",
        "        # if the count is less than 50, we take the whole set and randomly choose the remaining numbers again \n",
        "        if len(query_pool) >= 50:\n",
        "            query_list.extend(sample(query_pool, k=50))\n",
        "        else:\n",
        "            query_list.extend(query_pool)\n",
        "            query_list.extend(choices(list(query_pool), k=50-len(query_pool)))\n",
        "\n",
        "            if train_set['intent']=='covid_numbers':\n",
        "                # for this intent we only have the state name \"Delhi\" in our hand-written data set, but we dont want\n",
        "                # to make the training data biased, so we replace \"Delhi\" with a randomly taken state name\n",
        "                query_list = [ql.replace('delhi', choice(STATE_LIST)) for ql in query_list]\n",
        "    \n",
        "    training_data_big.append({'intent' : train_set['intent'], 'query' : query_list,\n",
        "                                'response' : train_set['response']})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbZ678AZy67u"
      },
      "source": [
        "file_path = '/content/drive/MyDrive/Colab Notebooks/VICCI/data/generated_train_data.json'\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write(json.dumps(training_data_big, indent=4))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}